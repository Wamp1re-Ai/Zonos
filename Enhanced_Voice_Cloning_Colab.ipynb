{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ðŸŽ¤ Enhanced Voice Cloning with Zonos TTS - Google Colab\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Wamp1re-Ai/Zonos/blob/main/Enhanced_Voice_Cloning_Colab.ipynb)\n",
    "\n",
    "This Google Colab notebook provides an **enhanced voice cloning system** using Zonos TTS. It's designed for ease of use within the Colab environment and offers several improvements over standard voice cloning approaches, focusing on naturalness, consistency, and control.\n",
    "\n",
    "**Key improvements include:**\n",
    "- âœ… Smooth, natural speech flow (reduced unnatural pauses and timing issues).\n",
    "- âœ… Consistent speaking rate.\n",
    "- âœ… Clear, intelligible speech (reduced gibberish generation).\n",
    "- âœ… Stable voice reproduction.\n",
    "\n",
    "## ðŸš€ Features:\n",
    "- ðŸ”§ **Advanced Audio Preprocessing**: Automatic silence removal and normalization for uploaded voice samples.\n",
    "- ðŸ“Š **Voice Quality Analysis**: SNR estimation and quality scoring for your voice samples.\n",
    "- âš™ï¸ **Optimized & Customizable Parameters**: Choose from Quality Presets for balanced results or fine-tune for specific needs. Includes options for faster generation (lower CFG Scales) and emotional expressiveness.\n",
    "- ðŸŽ¯ **Adaptive Settings**: Parameters automatically adjust based on the quality of your voice sample and chosen preset.\n",
    "- ðŸ”„ **Reproducible Results**: Seed support for consistent audio generation.\n",
    "- ðŸ“œ **Long Text Handling**: Automatically segments long texts into manageable chunks for continuous audio generation with prefixing.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Instructions:\n",
    "1. **Run Cell 1 (Setup)**: Clones the Zonos repository and sets up the Colab environment.\n",
    "2. **Run Cell 2 (Install Dependencies)**: Installs necessary Python packages using UV for speed.\n",
    "3. **Run Cell 3 (Load Model)**: Loads the Zonos TTS model. **IMPORTANT:** If you modify underlying model code (e.g., `zonos/model.py`), you MUST re-run this cell for changes to take effect.\n",
    "4. **Run Cell 4 (Helper Functions for Long Audio - NEW)**: Defines utility functions for text segmentation and chunked audio generation. This cell must be run before Cell 5.\n",
    "5. **Run Cell 5 (Upload Voice Sample)**: Upload a 10-30 second audio file of the voice you want to clone.\n",
    "6. **Run Cell 6 (Generate Speech)**: Generate speech using your cloned voice and selected Quality Preset. Handles long texts automatically.\n",
    "7. **Run Cell 7 (Run Benchmarks - Optional)**: Test generation speed and quality with different CFG Scales.\n",
    "\n",
    "**Troubleshooting Note**: If you encounter NumPy-related errors, especially after installing dependencies, try restarting the Colab Runtime (`Runtime` > `Restart runtime` or `Factory reset runtime`) and then re-run cells from Cell 1. This usually resolves such issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "setup_and_clone"
   },
   "outputs": [],
   "source": [
    "#@title 1. ðŸ“¥ Setup and Clone Repository\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"ðŸš€ Enhanced Voice Cloning Setup\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ… Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"âš ï¸ Not running in Google Colab\")\n",
    "\n",
    "# Clone the repository if it doesn't exist\n",
    "if not os.path.exists('Zonos'):\n",
    "    print(\"\\nðŸ“¥ Cloning Zonos repository...\")\n",
    "    !git clone https://github.com/Wamp1re-Ai/Zonos.git\n",
    "    print(\"âœ… Repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"\\nâœ… Repository already exists!\")\n",
    "\n",
    "# Change to the Zonos directory\n",
    "%cd Zonos\n",
    "\n",
    "# Install system dependencies\n",
    "print(\"\\nðŸ”§ Installing system dependencies...\")\n",
    "!apt-get update -qq\n",
    "!apt-get install -y espeak-ng git-lfs -qq\n",
    "!git lfs install\n",
    "print(\"âœ… System dependencies installed!\")\n",
    "\n",
    "# Check for enhanced files\n",
    "if os.path.exists('enhanced_voice_cloning.py'):\n",
    "    print(\"\\nðŸš€ Enhanced voice cloning files detected!\")\n",
    "    print(\"You have access to all the latest improvements.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Enhanced files not found. Using standard voice cloning.\")\n",
    "\n",
    "print(\"\\nâœ… Setup complete! Continue to Cell 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "#@title 2. âš¡ Install Dependencies with UV (Ultra-Fast Installation)\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"âš¡ Ultra-Fast Dependency Installation with UV\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Install UV for ultra-fast package management\n",
    "print(\"\\nðŸš€ Step 1: Installing UV (Rust-based package manager)...\")\n",
    "try:\n",
    "    # Check if uv is already installed\n",
    "    result = subprocess.run(['uv', '--version'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"âœ… UV already installed: {result.stdout.strip()}\")\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "except (FileNotFoundError, subprocess.CalledProcessError):\n",
    "    print(\"ðŸ“¦ Installing UV...\")\n",
    "    !curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "    # Add uv to PATH for current session\n",
    "    os.environ['PATH'] = f\"/root/.cargo/bin:{os.environ.get('PATH', '')}\"\n",
    "    print(\"âœ… UV installed successfully!\")\n",
    "\n",
    "# Step 2: Fix NumPy compatibility FIRST\n",
    "print(\"\\nðŸ”§ Step 2: Fixing NumPy compatibility (ultra-fast)...\")\n",
    "!uv pip install \"numpy==1.26.4\" --force-reinstall --system\n",
    "\n",
    "# Verify NumPy installation\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"âœ… NumPy {np.__version__} installed successfully\")\n",
    "    \n",
    "    # Double-check version\n",
    "    numpy_major = int(np.__version__.split('.')[0])\n",
    "    if numpy_major >= 2:\n",
    "        print(\"âš ï¸ NumPy 2.x still detected. This may require a runtime restart.\")\n",
    "        print(\"If you get errors in Cell 3, restart runtime and try again.\")\n",
    "    else:\n",
    "        print(\"âœ… NumPy version is now compatible with transformers\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ NumPy verification failed: {e}\")\n",
    "    print(\"Continuing with installation...\")\n",
    "\n",
    "# Step 3: Install core dependencies with UV (much faster)\n",
    "print(\"\\nâš¡ Step 3: Installing core dependencies with UV...\")\n",
    "\n",
    "# Check PyTorch (usually pre-installed in Colab)\n",
    "try:\n",
    "    import torch\n",
    "    import torchaudio\n",
    "    print(f\"âœ… PyTorch {torch.__version__} already available\")\n",
    "    print(f\"âœ… TorchAudio {torchaudio.__version__} already available\")\n",
    "except ImportError:\n",
    "    print(\"ðŸ“¦ Installing PyTorch with UV...\")\n",
    "    !uv pip install torch torchaudio --system\n",
    "\n",
    "# Install all other packages in one UV command (much faster than pip)\n",
    "print(\"âš¡ Installing all dependencies with UV (10x faster than pip)...\")\n",
    "!uv pip install \"transformers>=4.45.0,<4.50.0\" \"huggingface-hub>=0.20.0\" \"soundfile>=0.12.1\" \"phonemizer>=3.2.0\" \"inflect>=7.0.0\" \"scipy\" \"ipywidgets>=8.0.0\" \"nltk>=3.8\" --system\n",
    "\n",
    "print(\"\\nâš¡ Step 4: Installing Zonos package with UV...\")\n",
    "try:\n",
    "    !uv pip install -e . --system\n",
    "    print(\"âœ… Zonos package installed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Package installation failed, adding to Python path...\")\n",
    "    current_dir = os.getcwd()\n",
    "    if current_dir not in sys.path:\n",
    "        sys.path.insert(0, current_dir)\n",
    "    print(f\"âœ… Added {current_dir} to Python path\")\n",
    "\n",
    "installation_time = time.time() - start_time\n",
    "print(f\"\\nðŸŽ‰ All dependencies installed successfully in {installation_time:.1f} seconds!\")\n",
    "print(f\"âš¡ UV is ~10x faster than pip for package installation\")\n",
    "print(\"\\nðŸš€ Ready for Cell 3: Load Model\")\n",
    "print(\"\\nðŸ’¡ Note: If Cell 3 gives NumPy errors:\")\n",
    "print(\"   1. Runtime â†’ Restart runtime\")\n",
    "print(\"   2. Re-run Cell 1 and Cell 2\")\n",
    "print(\"   3. Then run Cell 3 again\")\n",
    "print(\"   This is normal and fixes the NumPy compatibility issue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "#@title 3. ðŸ¤– Load Enhanced Zonos Model\n",
    "# IMPORTANT: If you have modified the underlying Zonos Python files (e.g., zonos/model.py),\n",
    "# you MUST re-run this cell for those changes to take effect in the model.\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"ðŸ¤– Loading Enhanced Zonos Model\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Make sure we can import zonos modules\n",
    "current_dir = os.getcwd()\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "\n",
    "# Check NumPy version (should be fixed by Cell 2)\n",
    "print(\"ðŸ”§ Verifying NumPy compatibility...\")\n",
    "try:\n",
    "    import numpy as np\n",
    "    numpy_version = np.__version__\n",
    "    numpy_major = int(numpy_version.split('.')[0])\n",
    "    print(f\"NumPy version: {numpy_version}\")\n",
    "    \n",
    "    if numpy_major >= 2:\n",
    "        print(\"\\nâš ï¸ WARNING: NumPy 2.x detected!\")\n",
    "        print(\"This may cause issues. If you get errors below:\")\n",
    "        print(\"1. Runtime â†’ Restart runtime\")\n",
    "        print(\"2. Re-run Cell 1 and Cell 2\")\n",
    "        print(\"3. Try Cell 3 again\")\n",
    "        print(\"\\nContinuing anyway...\")\n",
    "    else:\n",
    "        print(\"âœ… NumPy version is compatible\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"âŒ NumPy not found! Please run Cell 2 first.\")\n",
    "    raise\n",
    "\n",
    "# Import PyTorch\n",
    "print(\"\\nðŸ“¦ Loading PyTorch...\")\n",
    "try:\n",
    "    import torch\n",
    "    import torchaudio\n",
    "    print(f\"âœ… PyTorch {torch.__version__}\")\n",
    "    print(f\"âœ… TorchAudio {torchaudio.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ PyTorch error: {e}\")\n",
    "    print(\"Please run Cell 2 to install dependencies.\")\n",
    "    raise\n",
    "\n",
    "# Import transformers with better error handling\n",
    "print(\"\\nðŸ¤— Loading Transformers...\")\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"âœ… Transformers {transformers.__version__}\")\n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    print(f\"âŒ Transformers error: {e}\")\n",
    "    \n",
    "    if \"numpy\" in error_msg.lower() or \"_center\" in error_msg:\n",
    "        print(\"\\nðŸ”§ This is the NumPy 2.x compatibility issue!\")\n",
    "        print(\"\\nðŸ“‹ SOLUTION:\")\n",
    "        print(\"1. Runtime â†’ Restart runtime\")\n",
    "        print(\"2. Run Cell 1 (Setup)\")\n",
    "        print(\"3. Run Cell 2 (Dependencies)\")\n",
    "        print(\"4. Run Cell 3 (this cell) again\")\n",
    "        print(\"\\nThis will fix the NumPy compatibility issue.\")\n",
    "    else:\n",
    "        print(\"Please check your dependencies in Cell 2.\")\n",
    "    raise\n",
    "\n",
    "# Try to import enhanced voice cloning modules\n",
    "print(\"\\nðŸš€ Loading Enhanced Voice Cloning...\")\n",
    "ENHANCED_AVAILABLE = False\n",
    "try:\n",
    "    # First check if the file exists\n",
    "    import os\n",
    "    if os.path.exists('enhanced_voice_cloning.py'):\n",
    "        print(\"âœ“ Enhanced voice cloning file found\")\n",
    "        \n",
    "        # Try importing the enhanced modules\n",
    "        from enhanced_voice_cloning import (\n",
    "            EnhancedVoiceCloner, \n",
    "            create_enhanced_voice_cloner, \n",
    "            quick_voice_clone\n",
    "        )\n",
    "        print(\"âœ… Enhanced Voice Cloning modules loaded successfully!\")\n",
    "        ENHANCED_AVAILABLE = True\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸ enhanced_voice_cloning.py not found in current directory\")\n",
    "        ENHANCED_AVAILABLE = False\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Enhanced modules import failed: {e}\")\n",
    "    print(\"This might be due to missing dependencies in the enhanced module.\")\n",
    "    print(\"Using standard voice cloning instead.\")\n",
    "    ENHANCED_AVAILABLE = False\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Unexpected error loading enhanced modules: {e}\")\n",
    "    print(\"Using standard voice cloning instead.\")\n",
    "    ENHANCED_AVAILABLE = False\n",
    "\n",
    "# Import standard Zonos modules\n",
    "print(\"\\nðŸŽµ Loading Zonos modules...\")\n",
    "try:\n",
    "    from zonos.model import Zonos\n",
    "    from zonos.conditioning import make_cond_dict, supported_language_codes\n",
    "    from zonos.utils import DEFAULT_DEVICE\n",
    "    print(\"âœ… Zonos modules loaded successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Zonos import error: {e}\")\n",
    "    print(\"Make sure Cell 2 completed successfully.\")\n",
    "    raise\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nðŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Load the model\n",
    "model_name = \"Zyphra/Zonos-v0.1-transformer\"\n",
    "print(f\"\\nðŸ“¥ Loading model: {model_name}\")\n",
    "print(\"This may take 2-5 minutes for the first time...\")\n",
    "\n",
    "try:\n",
    "    model = Zonos.from_pretrained(model_name, device=device)\n",
    "    model.requires_grad_(False).eval()\n",
    "    print(\"âœ… Model loaded successfully!\")\n",
    "    \n",
    "    # Model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nðŸ“Š Model Info:\")\n",
    "    print(f\"  - Parameters: {total_params:,}\")\n",
    "    print(f\"  - Device: {next(model.parameters()).device}\")\n",
    "    print(f\"  - Enhanced features: {'âœ… Available' if ENHANCED_AVAILABLE else 'âŒ Standard only'}\")\n",
    "    print(f\"  - Languages: {len(supported_language_codes)} supported\")\n",
    "    \n",
    "    # Create enhanced cloner if available\n",
    "    if ENHANCED_AVAILABLE:\n",
    "        print(\"\\nðŸš€ Creating Enhanced Voice Cloner...\")\n",
    "        try:\n",
    "            try:\n",
    "                enhanced_cloner = create_enhanced_voice_cloner(model=model, device=device)\n",
    "            except TypeError:\n",
    "                 print(\"  (Enhanced cloner does not accept model directly, creating with device only)\")\n",
    "                 enhanced_cloner = create_enhanced_voice_cloner(device=device)\n",
    "            print(\"âœ… Enhanced Voice Cloner ready!\")\n",
    "            globals()['enhanced_cloner'] = enhanced_cloner\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to create enhanced cloner: {e}\")\n",
    "            print(\"Will create fallback enhanced functions...\")\n",
    "            ENHANCED_AVAILABLE = False \n",
    "    \n",
    "    if not ENHANCED_AVAILABLE:\n",
    "        print(\"\\nðŸ”§ Creating fallback enhanced voice cloning functions...\")\n",
    "        try:\n",
    "            from zonos.speaker_cloning import (\n",
    "                preprocess_audio_for_cloning,\n",
    "                analyze_voice_quality,\n",
    "                get_voice_cloning_conditioning_params,\n",
    "                get_voice_cloning_sampling_params\n",
    "            )\n",
    "            \n",
    "            def simple_enhanced_clone_voice(wav, sr, **kwargs):\n",
    "                processed_wav = preprocess_audio_for_cloning(\n",
    "                    wav, sr,\n",
    "                    target_length_seconds=kwargs.get('target_length_seconds', 20.0),\n",
    "                    normalize=kwargs.get('normalize', True),\n",
    "                    remove_silence=kwargs.get('remove_silence', True)\n",
    "                )\n",
    "                quality_metrics = analyze_voice_quality(processed_wav, sr)\n",
    "                speaker_embedding = model.make_speaker_embedding(processed_wav, sr)\n",
    "                speaker_embedding = speaker_embedding.to(device, dtype=torch.bfloat16)\n",
    "                return speaker_embedding, quality_metrics\n",
    "            \n",
    "            def simple_enhanced_generate_speech(text, speaker_embedding=None, language='en-us', \n",
    "                                               voice_quality=None, seed=None, cfg_scale=2.0, \n",
    "                                               custom_conditioning_params=None, custom_sampling_params=None, \n",
    "                                               emotion_vector=None, **kwargs):\n",
    "                if seed is not None:\n",
    "                    torch.manual_seed(seed)\n",
    "                conditioning_params = get_voice_cloning_conditioning_params(voice_quality)\n",
    "                sampling_params = get_voice_cloning_sampling_params(voice_quality)\n",
    "                if custom_conditioning_params:\n",
    "                    conditioning_params.update(custom_conditioning_params)\n",
    "                if custom_sampling_params:\n",
    "                    sampling_params.update(custom_sampling_params)\n",
    "                \n",
    "                cond_dict_extra_args = {}\n",
    "                if emotion_vector is not None:\n",
    "                    cond_dict_extra_args['emotion'] = emotion_vector\n",
    "                    \n",
    "                cond_dict = make_cond_dict(\n",
    "                    text=text, language=language, speaker=speaker_embedding,\n",
    "                    device=device, **conditioning_params, **cond_dict_extra_args\n",
    "                )\n",
    "                conditioning = model.prepare_conditioning(cond_dict, cfg_scale=cfg_scale)\n",
    "                sampling_dict = {k: v for k, v in sampling_params.items() if k in ['min_p', 'top_k', 'top_p', 'temperature', 'repetition_penalty']}\n",
    "                \n",
    "                tokens_per_char = 20\n",
    "                estimated_tokens = len(text) * tokens_per_char\n",
    "                min_tokens = 1000\n",
    "                max_tokens = max(min_tokens, min(estimated_tokens, 86 * 120))\n",
    "                \n",
    "                codes = model.generate(\n",
    "                    prefix_conditioning=conditioning,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                    cfg_scale=cfg_scale, \n",
    "                    batch_size=1, \n",
    "                    progress_bar=True,\n",
    "                    sampling_params=sampling_dict\n",
    "                )\n",
    "                audio = model.autoencoder.decode(codes).cpu().detach()\n",
    "                return audio\n",
    "            \n",
    "            globals()['enhanced_clone_voice_from_audio'] = simple_enhanced_clone_voice\n",
    "            globals()['enhanced_generate_speech'] = simple_enhanced_generate_speech\n",
    "            print(\"âœ… Fallback enhanced functions created (now emotion-aware)!\")\n",
    "            ENHANCED_AVAILABLE = True \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to create fallback functions: {e}\")\n",
    "            print(\"Using standard voice cloning only.\")\n",
    "            ENHANCED_AVAILABLE = False \n",
    "    \n",
    "    globals()['model'] = model\n",
    "    globals()['device'] = device\n",
    "    globals()['ENHANCED_AVAILABLE'] = ENHANCED_AVAILABLE \n",
    "    \n",
    "    print(\"\\nðŸŽ‰ Setup complete! Ready for voice cloning.\")\n",
    "    print(\"\\nðŸš€ Next: Run Cell 4 (Helper Functions) then Cell 5 (Upload Voice Sample).\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading model: {e}\")\n",
    "    print(\"\\nðŸ”§ Troubleshooting:\")\n",
    "    print(\"1. Check internet connection\")\n",
    "    print(\"2. Restart runtime if NumPy issues persist\")\n",
    "    print(\"3. Re-run all cells from the beginning\")\n",
    "    raise"
   ]
  },
  { # New Helper Cell - Index 3
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "long_audio_helper_cell",
    "title": "4. ðŸ“œ Helper Functions for Long Audio & Text Segmentation"
   },
   "outputs": [],
   "source": [
    "#@title 4. ðŸ“œ Helper Functions for Long Audio & Text Segmentation\n",
    "#@markdown This cell defines utility functions for segmenting long text and generating audio in chunks. \n",
    "#@markdown **Run this cell once after loading the model (Cell 3) and before generating speech (Cell 6).**\n",
    "import nltk\n",
    "import torch\n",
    "import torchaudio \n",
    "import time\n",
    "\n",
    "def setup_nltk_punkt():\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except nltk.downloader.DownloadError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    print(\"âœ… NLTK 'punkt' tokenizer is available.\")\n",
    "\n",
    "def segment_text_into_chunks(text: str, max_chars_per_chunk: int = 600, max_words_per_chunk: int = 100) -> list[str]:\n",
    "    setup_nltk_punkt()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if not sentences: return []\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk_text = \"\"\n",
    "    current_chunk_char_count = 0\n",
    "    current_chunk_word_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_char_count = len(sentence)\n",
    "        sentence_word_count = len(sentence.split())\n",
    "\n",
    "        if not current_chunk_text and \\\n",
    "           (sentence_char_count > max_chars_per_chunk or sentence_word_count > max_words_per_chunk):\n",
    "            chunks.append(sentence)\n",
    "            continue\n",
    "\n",
    "        space_char_needed = 1 if current_chunk_text else 0\n",
    "        if current_chunk_text and \\\n",
    "           ((current_chunk_char_count + sentence_char_count + space_char_needed > max_chars_per_chunk) or \\\n",
    "            (current_chunk_word_count + sentence_word_count > max_words_per_chunk)):\n",
    "            chunks.append(current_chunk_text)\n",
    "            current_chunk_text = sentence\n",
    "            current_chunk_char_count = sentence_char_count\n",
    "            current_chunk_word_count = sentence_word_count\n",
    "        else:\n",
    "            if not current_chunk_text:\n",
    "                current_chunk_text = sentence\n",
    "                current_chunk_char_count = sentence_char_count\n",
    "                current_chunk_word_count = sentence_word_count\n",
    "            else:\n",
    "                current_chunk_text += \" \" + sentence\n",
    "                current_chunk_char_count += (sentence_char_count + space_char_needed) \n",
    "                current_chunk_word_count += sentence_word_count\n",
    "    \n",
    "    if current_chunk_text: chunks.append(current_chunk_text)\n",
    "    return chunks\n",
    "\n",
    "def generate_long_audio_from_text(\n",
    "    full_text: str, \n",
    "    model, \n",
    "    device: torch.device, \n",
    "    speaker_embedding: torch.Tensor, \n",
    "    language: str, \n",
    "    cfg_scale: float,\n",
    "    custom_conditioning_params: dict, \n",
    "    custom_sampling_params: dict, \n",
    "    seed: int,\n",
    "    make_cond_dict_func, \n",
    "    max_chars_per_chunk: int = 500, \n",
    "    max_words_per_chunk: int = 80,  \n",
    "    prefix_duration_seconds: float = 1.5,\n",
    "    max_new_tokens_per_chunk: int = 86 * 30 \n",
    ") -> tuple[torch.Tensor | None, int]:\n",
    "    print(f\"ðŸŽ™ï¸ Starting long audio generation for text ({len(full_text)} chars)... Using chunked method.\")\n",
    "    \n",
    "    text_chunks = segment_text_into_chunks(full_text, max_chars_per_chunk, max_words_per_chunk)\n",
    "    if not text_chunks:\n",
    "        print(\"No text chunks to process.\")\n",
    "        return None, model.autoencoder.sampling_rate\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    full_audio_parts = []\n",
    "    previous_chunk_audio_wav_cpu = None \n",
    "    sample_rate = model.autoencoder.sampling_rate\n",
    "\n",
    "    for i, chunk_text in enumerate(text_chunks):\n",
    "        print(f\"\\nGenerating chunk {i+1}/{len(text_chunks)}: \\\"{chunk_text[:80]}...\\\"\")\n",
    "        \n",
    "        audio_prefix_codes = None\n",
    "        if previous_chunk_audio_wav_cpu is not None and previous_chunk_audio_wav_cpu.numel() > 0:\n",
    "            prefix_num_samples = int(prefix_duration_seconds * sample_rate)\n",
    "            current_prefix_audio = previous_chunk_audio_wav_cpu\n",
    "            if current_prefix_audio.dim() == 2 and current_prefix_audio.shape[0] == 1:\n",
    "                current_prefix_audio = current_prefix_audio.squeeze(0)\n",
    "\n",
    "            if current_prefix_audio.shape[-1] > prefix_num_samples:\n",
    "                audio_prefix_snippet = current_prefix_audio[-prefix_num_samples:]\n",
    "            else:\n",
    "                audio_prefix_snippet = current_prefix_audio\n",
    "            \n",
    "            print(f\"  Using audio prefix of {audio_prefix_snippet.shape[-1]/sample_rate:.2f}s from previous chunk.\")\n",
    "            audio_prefix_snippet_for_encode = audio_prefix_snippet.to(device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            if audio_prefix_snippet_for_encode.numel() > 0 and audio_prefix_snippet_for_encode.shape[-1] > 0 :\n",
    "                try:\n",
    "                    with torch.no_grad():\n",
    "                         audio_prefix_codes_encoded = model.autoencoder.encode(audio_prefix_snippet_for_encode)\n",
    "                    if audio_prefix_codes_encoded.numel() > 0:\n",
    "                        audio_prefix_codes = audio_prefix_codes_encoded\n",
    "                        print(f\"    Encoded audio prefix: {audio_prefix_codes.shape}\") \n",
    "                    else:\n",
    "                        print(\"    Warning: Encoding audio prefix resulted in empty tensor. Skipping prefix.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    Error encoding audio prefix: {e}. Skipping prefix for this chunk.\")\n",
    "            else:\n",
    "                print(\"    Warning: Audio prefix snippet is empty or invalid. Skipping prefix.\")\n",
    "        \n",
    "        cond_dict = make_cond_dict_func(\n",
    "            text=chunk_text, language=language, speaker=speaker_embedding,\n",
    "            device=device, **custom_conditioning_params \n",
    "        )\n",
    "        prepared_conditioning = model.prepare_conditioning(cond_dict, cfg_scale=cfg_scale)\n",
    "\n",
    "        print(f\"  Generating with CFG: {cfg_scale}, Max Tokens: {max_new_tokens_per_chunk}\")\n",
    "        start_gen_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            generated_codes = model.generate(\n",
    "                prefix_conditioning=prepared_conditioning,\n",
    "                audio_prefix_codes=audio_prefix_codes,\n",
    "                max_new_tokens=max_new_tokens_per_chunk,\n",
    "                cfg_scale=cfg_scale,\n",
    "                batch_size=1, \n",
    "                sampling_params=custom_sampling_params,\n",
    "                progress_bar=True \n",
    "            )\n",
    "            current_chunk_audio_wav = model.autoencoder.decode(generated_codes).squeeze(0).squeeze(0).cpu()\n",
    "        end_gen_time = time.time()\n",
    "        if current_chunk_audio_wav.dim() == 0: current_chunk_audio_wav = current_chunk_audio_wav.unsqueeze(0)\n",
    "            \n",
    "        print(f\"  Chunk generation time: {end_gen_time - start_gen_time:.2f}s, Audio duration: {current_chunk_audio_wav.shape[-1]/sample_rate:.2f}s\")\n",
    "\n",
    "        if current_chunk_audio_wav.numel() > 0:\n",
    "            full_audio_parts.append(current_chunk_audio_wav)\n",
    "            previous_chunk_audio_wav_cpu = current_chunk_audio_wav\n",
    "        else:\n",
    "            print(f\"  Warning: Chunk {i+1} resulted in empty audio. Not using for prefixing next chunk.\")\n",
    "    \n",
    "    if not full_audio_parts:\n",
    "        print(\"No audio parts were generated.\")\n",
    "        return None, sample_rate\n",
    "        \n",
    "    full_audio_output = torch.cat(full_audio_parts, dim=-1)\n",
    "    print(f\"\\nðŸŽ‰ Long audio generation complete. Total duration: {full_audio_output.shape[-1]/sample_rate:.2f}s\")\n",
    "    return full_audio_output.unsqueeze(0), sample_rate\n",
    "\n",
    "print(\"âœ… Helper functions for text segmentation and long audio generation are defined.\")\n",
    "setup_nltk_punkt() # Call once to ensure 'punkt' is ready when this cell runs"
   ]
  },
  { # Original Cell 4 (Upload Voice) - Now Index 4
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "upload_voice"
   },
   "outputs": [],
   "source": [
    "#@title 5. ðŸŽ¤ Upload Voice Sample for Cloning\n", # Title updated to reflect new cell order
    "from google.colab import files\n",
    "import torchaudio\n",
    "import torch\n",
    "import IPython.display as ipd\n",
    "\n",
    "print(\"ðŸŽ¤ Voice Cloning - Upload Your Audio File\")\n",
    "print(\"Upload an audio file (10-30 seconds) to clone the speaker's voice\")\n",
    "print(\"Supported formats: WAV, MP3, FLAC, etc.\")\n",
    "print(\"\")\n",
    "\n",
    "# Upload audio file\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    # Get the uploaded file\n",
    "    audio_file = list(uploaded.keys())[0]\n",
    "    print(f\"\\nðŸ“ Processing: {audio_file}\")\n",
    "    \n",
    "    try:\n",
    "        # Load and process the audio\n",
    "        wav, sr = torchaudio.load(audio_file)\n",
    "        \n",
    "        # Convert to mono if needed\n",
    "        if wav.shape[0] > 1:\n",
    "            wav = wav.mean(0, keepdim=True)\n",
    "        \n",
    "        # Show audio info\n",
    "        duration = wav.shape[1] / sr\n",
    "        print(f\"ðŸ“Š Audio Info:\")\n",
    "        print(f\"  - Duration: {duration:.1f} seconds\")\n",
    "        print(f\"  - Sample rate: {sr} Hz\")\n",
    "        print(f\"  - Channels: {wav.shape[0]}\")\n",
    "        \n",
    "        # Quality recommendations\n",
    "        if duration < 5:\n",
    "            print(\"\\nâš ï¸ Audio is quite short (< 5s). Consider using 10-20 seconds for better results.\")\n",
    "        elif duration > 30:\n",
    "            print(\"\\nðŸ’¡ Audio is long (> 30s). The system will use the best portion automatically.\")\n",
    "        else:\n",
    "            print(\"\\nâœ… Audio duration is optimal for voice cloning!\")\n",
    "        \n",
    "        # Play the audio\n",
    "        print(\"\\nðŸ”Š Preview of your audio:\")\n",
    "        ipd.display(ipd.Audio(wav.numpy(), rate=sr))\n",
    "        \n",
    "        # Create speaker embedding\n",
    "        print(\"\\nðŸ§  Creating voice embedding...\")\n",
    "        \n",
    "        if ENHANCED_AVAILABLE and 'enhanced_cloner' in globals():\n",
    "            print(\"ðŸš€ Using Enhanced Voice Cloner class...\")\n",
    "            speaker_embedding, quality_metrics = enhanced_cloner.clone_voice_from_audio(\n",
    "                wav, sr,\n",
    "                target_length_seconds=min(20.0, duration),\n",
    "                normalize=True,\n",
    "                remove_silence=True,\n",
    "                analyze_quality=True\n",
    "            )\n",
    "            print(f\"\\nðŸ“ˆ Voice Quality Analysis:\")\n",
    "            print(f\"  - Quality Score: {quality_metrics['quality_score']:.3f} / 1.000\")\n",
    "            print(f\"  - SNR Estimate: {quality_metrics['snr_estimate']:.1f} dB\")\n",
    "            globals()['voice_quality_metrics'] = quality_metrics\n",
    "        elif ENHANCED_AVAILABLE and 'enhanced_clone_voice_from_audio' in globals():\n",
    "            print(\"ðŸš€ Using fallback enhanced_clone_voice_from_audio function...\")\n",
    "            speaker_embedding, quality_metrics = enhanced_clone_voice_from_audio(\n",
    "                wav, sr,\n",
    "                target_length_seconds=min(20.0, duration),\n",
    "                normalize=True,\n",
    "                remove_silence=True\n",
    "            )\n",
    "            print(f\"\\nðŸ“ˆ Voice Quality Analysis (from fallback):\")\n",
    "            print(f\"  - Quality Score: {quality_metrics['quality_score']:.3f} / 1.000\")\n",
    "            print(f\"  - SNR Estimate: {quality_metrics['snr_estimate']:.1f} dB\")\n",
    "            globals()['voice_quality_metrics'] = quality_metrics\n",
    "        else:\n",
    "            print(\"ðŸ“¢ Using standard Zonos model.make_speaker_embedding...\")\n",
    "            speaker_embedding = model.make_speaker_embedding(wav, sr)\n",
    "            speaker_embedding = speaker_embedding.to(device, dtype=torch.bfloat16)\n",
    "            globals()['voice_quality_metrics'] = {} \n",
    "        \n",
    "        globals()['cloned_voice'] = speaker_embedding\n",
    "        globals()['original_audio_file'] = audio_file\n",
    "        \n",
    "        print(\"\\nâœ… Voice cloning successful!\")\n",
    "        print(\"Your cloned voice is ready to use in Cell 6 (Generate Speech).\")\n", # Updated next cell number
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing audio: {e}\")\n",
    "        print(\"Please try a different audio file or check the format.\")\n",
    "else:\n",
    "    print(\"No file uploaded. You can still use the default voice in Cell 6 (Generate Speech).\")" # Updated next cell number
   ]
  },
  { # Original Cell 5 (Generate Speech) - Now Index 5
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "generate_speech" # ID remains same
   },
   "outputs": [],
   "source": [
    "#@title 6. ðŸŽ¤ Generate Speech with Enhanced Voice Cloning\n", # Title updated
    "import IPython.display as ipd\n",
    "import torch\n",
    "import time\n",
    "\n",
    "#@markdown ### Text and Settings\n",
    "#@markdown The system now automatically handles long input texts by breaking them into chunks for generation. This allows for much longer audio outputs.\n",
    "text = \"Hello! This is an enhanced voice cloning demonstration using Zonos TTS. The new system provides much better consistency and naturalness. For very long texts, it will automatically use a chunking method with audio prefixing to maintain continuity, though this might take a bit longer overall.\" #@param {type:\"string\"}\n",
    "language = \"en-us\" #@param [\"en-us\", \"en-gb\", \"fr-fr\", \"es-es\", \"de-de\", \"it-it\", \"ja-jp\", \"zh-cn\"]\n",
    "seed = 42 #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ### Voice Quality Preset\n",
    "#@markdown Select a preset to balance speed, quality, and expressiveness. Advanced settings are optimized based on your choice.\n",
    "quality_preset = \"Balanced\" #@param [\"Conservative\", \"Balanced\", \"Fast (Less Expressive)\", \"Expressive\", \"Creative\"]\n",
    "\n",
    "#@markdown **Quality Preset Descriptions:**\n",
    "#@markdown - **Conservative**: Safe, stable output with minimal artifacts. Good for challenging audio or when maximum clarity is needed.\n",
    "#@markdown - **Balanced**: Good balance of quality, naturalness, and speed (recommended starting point).\n",
    "#@markdown - **Fast (Less Expressive)**: Prioritizes generation speed by using a lower CFG Scale (1.5). Output may be flatter or less expressive but is significantly faster.\n",
    "#@markdown - **Expressive**: More dynamic and expressive speech, with an adjusted emotional profile for liveliness (e.g., slightly happier/more surprised).\n",
    "#@markdown - **Creative**: Experimental, most expressive but may have artifacts, with a unique, diverse emotional profile.\n",
    "#@markdown \n",
    "#@markdown *Underlying parameters like CFG Scale, pitch variation, speaking rate, and sampling settings (min_p, temperature) are automatically adjusted based on your voice sample's quality and the chosen preset. The 'Expressive' and 'Creative' presets also apply specific emotion vectors.*\n",
    "\n",
    "MAX_TEXT_LENGTH_FOR_SINGLE_PASS = 700 # Characters\n",
    "\n",
    "print(\"ðŸŽ¤ Enhanced Voice Cloning Generation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "speaker_embedding = globals().get('cloned_voice', None)\n",
    "if speaker_embedding is not None:\n",
    "    print(\"ðŸŽ­ Using your cloned voice!\")\n",
    "    if 'original_audio_file' in globals(): print(f\"ðŸ“ Voice source: {original_audio_file}\")\n",
    "else:\n",
    "    print(\"ðŸŽ¤ Using default voice (upload audio in Cell 5 to use your own voice)\")\n", # Updated previous cell number
    "\n",
    "print(f\"\\nðŸŽµ Generating speech...\")\n",
    "print(f\"ðŸ“ Text: {text[:100]}{'...' if len(text) > 100 else ''} (Total length: {len(text)} chars)\")\n",
    "print(f\"ðŸŒ Language: {language}\")\n",
    "print(f\"ðŸŽ² Seed: {seed}\")\n",
    "\n",
    "start_time_total = time.time()\n",
    "audio = None # Initialize audio variable\n",
    "final_sample_rate = model.autoencoder.sampling_rate # Default sample rate\n",
    "\n",
    "try:\n",
    "    # Determine parameters based on preset\n",
    "    voice_quality = globals().get('voice_quality_metrics', None)\n",
    "    quality_score = voice_quality.get('quality_score', 0.7) if voice_quality else 0.7\n",
    "    snr_estimate = voice_quality.get('snr_estimate', 20.0) if voice_quality else 20.0\n",
    "    emotion_vector_override = None\n",
    "\n",
    "    if quality_preset == \"Conservative\":\n",
    "        base_pitch = 8.0; base_rate = 10.0; base_min_p = 0.02; base_temp = 0.6; cfg_scale_notebook = 2.5\n",
    "    elif quality_preset == \"Fast (Less Expressive)\":\n",
    "        base_pitch = 8.0; base_rate = 10.0; base_min_p = 0.03; base_temp = 0.7; cfg_scale_notebook = 1.5\n",
    "    elif quality_preset == \"Expressive\":\n",
    "        base_pitch = 18.0; base_rate = 14.0; base_min_p = 0.06; base_temp = 0.85; cfg_scale_notebook = 2.0\n",
    "        emotion_vector_override = [0.6, 0.05, 0.05, 0.05, 0.1, 0.05, 0.05, 0.05]\n",
    "    elif quality_preset == \"Creative\":\n",
    "        base_pitch = 22.0; base_rate = 16.0; base_min_p = 0.08; base_temp = 0.95; cfg_scale_notebook = 1.8\n",
    "        emotion_vector_override = [0.2, 0.1, 0.1, 0.1, 0.2, 0.1, 0.1, 0.1]\n",
    "    else:  # Balanced (default)\n",
    "        base_pitch = 12.0; base_rate = 12.0; base_min_p = 0.04; base_temp = 0.75; cfg_scale_notebook = 2.2\n",
    "        \n",
    "    quality_factor = min(1.2, max(0.8, quality_score * 1.2))\n",
    "    snr_factor = min(1.1, max(0.9, (snr_estimate - 15.0) / 20.0 + 1.0))\n",
    "    pitch_std = max(5.0, min(25.0, base_pitch * quality_factor))\n",
    "    speaking_rate = max(8.0, min(18.0, base_rate * snr_factor))\n",
    "    min_p_val = max(0.01, min(0.15, base_min_p * quality_factor))\n",
    "    temperature_val = max(0.5, min(1.0, base_temp * quality_factor))\n",
    "    cfg_scale_notebook = max(1.0, min(3.0, cfg_scale_notebook))\n",
    "    \n",
    "    current_custom_conditioning_params = {'pitch_std': pitch_std, 'speaking_rate': speaking_rate}\n",
    "    if emotion_vector_override:\n",
    "        current_custom_conditioning_params['emotion'] = emotion_vector_override\n",
    "    current_custom_sampling_params = {'min_p': min_p_val, 'temperature': temperature_val}\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ Using {quality_preset} preset with CFG Scale: {cfg_scale_notebook:.1f}\")\n",
    "    if emotion_vector_override: print(f\"  Emotion Vector Override: {emotion_vector_override}\")\n",
    "\n",
    "    if len(text) > MAX_TEXT_LENGTH_FOR_SINGLE_PASS:\n",
    "        print(f\"\\nðŸ“œ Input text is long ({len(text)} chars). Using chunked generation method.\")\n",
    "        print(\"This may take longer but allows for extended audio output.\")\n",
    "        # Ensure make_cond_dict is available from Cell 3's globals\n",
    "        audio, final_sample_rate = generate_long_audio_from_text(\n",
    "            full_text=text, model=model, device=device, speaker_embedding=speaker_embedding,\n",
    "            language=language, cfg_scale=cfg_scale_notebook, \n",
    "            custom_conditioning_params=current_custom_conditioning_params, \n",
    "            custom_sampling_params=current_custom_sampling_params, \n",
    "            seed=seed, make_cond_dict_func=make_cond_dict \n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nGenerating audio in a single pass...\")\n",
    "        use_enhanced_cloner_class = ENHANCED_AVAILABLE and 'enhanced_cloner' in globals()\n",
    "        use_fallback_enhanced_func = ENHANCED_AVAILABLE and not use_enhanced_cloner_class and 'enhanced_generate_speech' in globals()\n",
    "\n",
    "        if use_fallback_enhanced_func:\n",
    "            print(\"ðŸš€ Using fallback enhanced_generate_speech function...\")\n",
    "            # This function already includes the logic to use custom_conditioning_params and emotion_vector\n",
    "            audio = enhanced_generate_speech(\n",
    "                text=text, speaker_embedding=speaker_embedding, language=language,\n",
    "                voice_quality=voice_quality, custom_conditioning_params=current_custom_conditioning_params, # Already includes emotion if set\n",
    "                custom_sampling_params=current_custom_sampling_params, cfg_scale=cfg_scale_notebook, seed=seed,\n",
    "                emotion_vector=emotion_vector_override # Explicitly pass for clarity, though it's in custom_conditioning_params\n",
    "            )\n",
    "            final_sample_rate = model.autoencoder.sampling_rate\n",
    "        elif use_enhanced_cloner_class:\n",
    "            print(\"ðŸš€ Using EnhancedVoiceCloner class...\")\n",
    "            # Ensure enhanced_cloner.generate_speech can handle emotion_vector if passed,\n",
    "            # or that it correctly uses it from custom_conditioning_params.\n",
    "            audio = enhanced_cloner.generate_speech(\n",
    "                text=text, speaker_embedding=speaker_embedding, language=language,\n",
    "                voice_quality=voice_quality, custom_conditioning_params=current_custom_conditioning_params, \n",
    "                custom_sampling_params=current_custom_sampling_params, cfg_scale=cfg_scale_notebook, seed=seed,\n",
    "                emotion_vector=emotion_vector_override \n",
    "            )\n",
    "            final_sample_rate = enhanced_cloner.model.autoencoder.sampling_rate \n",
    "        else: # Standard (non-enhanced) Zonos path\n",
    "            print(\"ðŸ“¢ Using standard Zonos model.generate (no enhanced features or emotion override)...\")\n",
    "            # Standard path doesn't use emotion_vector_override explicitly with make_cond_dict here\n",
    "            cond_dict = make_cond_dict(text=text, language=language, speaker=speaker_embedding, device=device, **current_custom_conditioning_params)\n",
    "            conditioning = model.prepare_conditioning(cond_dict, cfg_scale=cfg_scale_notebook)\n",
    "            tokens_per_char = 20\n",
    "            estimated_tokens = len(text) * tokens_per_char\n",
    "            min_tokens = 1000\n",
    "            max_tokens = max(min_tokens, min(estimated_tokens, 86 * 120))\n",
    "            codes = model.generate(\n",
    "                prefix_conditioning=conditioning, max_new_tokens=max_tokens,\n",
    "                cfg_scale=cfg_scale_notebook, batch_size=1, progress_bar=True,\n",
    "                sampling_params=current_custom_sampling_params\n",
    "            )\n",
    "            audio = model.autoencoder.decode(codes).cpu().detach()\n",
    "            final_sample_rate = model.autoencoder.sampling_rate\n",
    "        print(f\"âœ… Single-pass generation completed!\")\n",
    "    \n",
    "    if audio is not None and audio.numel() > 0:\n",
    "        if audio.dim() == 2 and audio.size(0) == 1: audio = audio.squeeze(0) # [1, T] -> [T]\n",
    "        elif audio.dim() == 3 and audio.size(0) == 1 and audio.size(1) == 1: audio = audio.squeeze() # [1,1,T] -> [T]\n",
    "        \n",
    "        generation_time = time.time() - start_time_total\n",
    "        duration = audio.shape[-1] / final_sample_rate\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Generation Stats:\")\n",
    "        print(f\"  - Total generation time: {generation_time:.2f} seconds\")\n",
    "        print(f\"  - Audio duration: {duration:.2f} seconds\")\n",
    "        print(f\"  - Sample rate: {final_sample_rate} Hz\")\n",
    "        \n",
    "        print(f\"\\nðŸ”Š Generated Audio:\")\n",
    "        # Ensure wav_numpy is 1D or 2D for ipd.Audio\n",
    "        wav_numpy = audio.cpu().numpy()\n",
    "        if wav_numpy.ndim == 1: wav_numpy = np.expand_dims(wav_numpy, axis=0) # [T] -> [1,T]\n",
    "        ipd.display(ipd.Audio(wav_numpy, rate=final_sample_rate))\n",
    "        globals()['last_generated_audio'] = (wav_numpy, final_sample_rate)\n",
    "        print(f\"\\nâœ… Success! Your voice clone is ready.\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Audio generation failed or produced empty output.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during audio generation: {e}\")\n",
    "    print(\"\\nðŸ”§ Troubleshooting:\")\n",
    "    print(\"- Try shorter text (under 200 characters for single pass).\")\n",
    "    print(\"- Check GPU memory usage (Runtime > View resources).\")\n",
    "    print(\"- Restart runtime if NumPy/other dependency issues persist (Runtime > Restart runtime).\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  { # Original Cell 6 (Benchmark) - Now Index 6
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "benchmark_cell_new",
    "title": "7. ðŸ“Š Run CFG Scale Benchmarks" # Title updated
   },
   "outputs": [],
   "source": [
    "#@title 7. ðŸ“Š Run CFG Scale Benchmarks\n", # Title updated
    "#@markdown This cell runs benchmarks with different CFG scales (1.0, 1.5, 2.2). Other generation parameters (pitch, rate, sampling) are based on the 'Balanced' preset to isolate the impact of CFG Scale.\n",
    "#@markdown - **CFG Scale 1.0**: Typically offers the fastest generation but may result in the least expressive or most robotic audio. \n",
    "#@markdown - **CFG Scale 1.5**: Used by the \"Fast (Less Expressive)\" preset in Cell 5. Aims for a balance between speed and quality, though still less expressive than higher CFG scales.\n",
    "#@markdown - **CFG Scale 2.2**: Default for the \"Balanced\" preset in Cell 5, offering a good blend of quality and naturalness.\n",
    "#@markdown Results will show Real-Time Factor (RTF), audio duration, generation time, and allow you to listen to each sample.\n",
    "#@markdown \n",
    "#@markdown **IMPORTANT:** If `zonos/model.py` (or other underlying model code) has been changed due to updates or local modifications, you **MUST re-run Cell 3 (Load Model)** to load the new model code *before* running these benchmarks or generating audio in Cell 5.\n",
    "\n",
    "import time\n",
    "import torchaudio\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "benchmark_audio_dir = \"/content/Zonos/benchmark_audio\"\n",
    "if not os.path.exists(benchmark_audio_dir):\n",
    "    os.makedirs(benchmark_audio_dir)\n",
    "\n",
    "def run_benchmark_trial(text_input, language_code, seed_value, cfg_scale_to_test, quality_preset_value, \n",
    "                        speaker_embedding_tensor, voice_quality_data, \n",
    "                        zonos_model, torch_device, \n",
    "                        run_warmup=False):\n",
    "    print(f\"\\n--- Benchmarking Trial ---\")\n",
    "    print(f\"Text: '{text_input[:50]}...' ({len(text_input)} chars)\")\n",
    "    print(f\"CFG Scale: {cfg_scale_to_test}, Preset (base for other params): {quality_preset_value}\")\n",
    "\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "    quality_score = voice_quality_data.get('quality_score', 0.7) if voice_quality_data else 0.7\n",
    "    snr_estimate = voice_quality_data.get('snr_estimate', 20.0) if voice_quality_data else 20.0\n",
    "\n",
    "    base_pitch, base_rate, base_min_p, base_temp = 12.0, 12.0, 0.04, 0.75\n",
    "\n",
    "    quality_factor = min(1.2, max(0.8, quality_score * 1.2))\n",
    "    snr_factor = min(1.1, max(0.9, (snr_estimate - 15.0) / 20.0 + 1.0))\n",
    "    \n",
    "    pitch_std = max(5.0, min(25.0, base_pitch * quality_factor))\n",
    "    speaking_rate = max(8.0, min(18.0, base_rate * snr_factor))\n",
    "    min_p_val = max(0.01, min(0.15, base_min_p * quality_factor))\n",
    "    temperature_val = max(0.5, min(1.0, base_temp * quality_factor))\n",
    "\n",
    "    current_custom_conditioning = {'pitch_std': pitch_std, 'speaking_rate': speaking_rate}\n",
    "    current_custom_sampling = {'min_p': min_p_val, 'temperature': temperature_val}\n",
    "\n",
    "    if run_warmup:\n",
    "        print(\"Running warmup...\")\n",
    "        warmup_text = \"Warmup.\"\n",
    "        warmup_cond_dict = make_cond_dict(\n",
    "            text=warmup_text, language=language_code, speaker=speaker_embedding_tensor,\n",
    "            device=torch_device, **current_custom_conditioning\n",
    "        )\n",
    "        warmup_conditioning = zonos_model.prepare_conditioning(warmup_cond_dict, cfg_scale=cfg_scale_to_test)\n",
    "        _ = zonos_model.generate(\n",
    "            prefix_conditioning=warmup_conditioning, max_new_tokens=30, cfg_scale=cfg_scale_to_test,\n",
    "            batch_size=1, sampling_params=current_custom_sampling, progress_bar=False\n",
    "        )\n",
    "        print(\"Warmup complete.\")\n",
    "\n",
    "    generation_start_time = time.time()\n",
    "    cond_dict = make_cond_dict(\n",
    "        text=text_input, language=language_code, speaker=speaker_embedding_tensor,\n",
    "        device=torch_device, **current_custom_conditioning \n",
    "    )\n",
    "    prepared_conditioning = zonos_model.prepare_conditioning(cond_dict, cfg_scale=cfg_scale_to_test)\n",
    "    \n",
    "    tokens_per_char = 15 \n",
    "    estimated_tokens = len(text_input) * tokens_per_char\n",
    "    min_gen_tokens = 200\n",
    "    max_gen_tokens = max(min_gen_tokens, min(estimated_tokens, 86 * 100))\n",
    "\n",
    "    codes = zonos_model.generate(\n",
    "        prefix_conditioning=prepared_conditioning, max_new_tokens=max_gen_tokens,\n",
    "        cfg_scale=cfg_scale_to_test, batch_size=1, \n",
    "        sampling_params=current_custom_sampling, progress_bar=True\n",
    "    )\n",
    "    audio_output = zonos_model.autoencoder.decode(codes).cpu().detach()\n",
    "    generation_time = time.time() - generation_start_time\n",
    "    sample_rate = zonos_model.autoencoder.sampling_rate\n",
    "    \n",
    "    if audio_output.dim() == 2 and audio_output.size(0) > 1: audio_output = audio_output[0:1, :]\n",
    "    audio_duration = audio_output.shape[-1] / sample_rate\n",
    "    rtf = generation_time / audio_duration if audio_duration > 0 else float('inf')\n",
    "    \n",
    "    print(f\"  Generated {audio_duration:.2f}s audio in {generation_time:.2f}s. RTF: {rtf:.2f}\")\n",
    "\n",
    "    clean_text_for_filename = text_input[:20].replace(' ', '_').replace('.', '').replace('!', '').replace('?', '')\n",
    "    audio_filename = f\"benchmark_cfg_{cfg_scale_to_test}_seed_{seed_value}_text_{clean_text_for_filename}.wav\"\n",
    "    audio_filepath = os.path.join(benchmark_audio_dir, audio_filename)\n",
    "    torchaudio.save(audio_filepath, audio_output.squeeze(0), sample_rate)\n",
    "    print(f\"  Saved audio to: {audio_filepath}\")\n",
    "    return rtf, audio_duration, generation_time, audio_filepath\n",
    "\n",
    "texts_to_benchmark = [\n",
    "    \"Hello world.\",\n",
    "    \"This is a test of the emergency broadcast system.\",\n",
    "    \"The quick brown fox jumps over the lazy dog, and other fables are often used for typing practice.\"\n",
    "]\n",
    "cfg_scales_to_benchmark = [1.0, 1.5, 2.2]\n",
    "benchmark_language = \"en-us\"\n",
    "benchmark_seed = 42 \n",
    "benchmark_quality_preset_for_other_params = \"Balanced\" \n",
    "benchmark_results_list = [] \n",
    "\n",
    "if 'model' not in globals() or 'device' not in globals():\n",
    "    print(\"âš ï¸ Model or device not found. Please run previous cells (1-3) to load the model.\")\n",
    "elif 'make_cond_dict' not in globals():\n",
    "    print(\"âš ï¸ make_cond_dict not found. Please ensure Cell 3 (model loading) has run successfully.\")\n",
    "else:\n",
    "    current_speaker_embedding = globals().get('cloned_voice', None)\n",
    "    if current_speaker_embedding is None: print(\"ðŸŽ¤ No cloned voice found. Using default speaker if model supports.\")\n",
    "    current_voice_quality_metrics = globals().get('voice_quality_metrics', {})\n",
    "    \n",
    "    print(\"\\nðŸ”¥ Running a single warm-up generation before benchmark loop (using CFG 2.2 from preset)...\")\n",
    "    run_benchmark_trial(\n",
    "        \"Warmup text.\", benchmark_language, benchmark_seed, 2.2, \n",
    "        benchmark_quality_preset_for_other_params, current_speaker_embedding, current_voice_quality_metrics,\n",
    "        model, device, run_warmup=False \n",
    "    )\n",
    "    print(\"ðŸ”¥ Warm-up finished.\\n\")\n",
    "\n",
    "    for cfg_val in cfg_scales_to_benchmark:\n",
    "        for text_sample in texts_to_benchmark:\n",
    "            rtf, audio_dur, gen_time, audio_file = run_benchmark_trial(\n",
    "                text_sample, benchmark_language, benchmark_seed, cfg_val,\n",
    "                benchmark_quality_preset_for_other_params, current_speaker_embedding, current_voice_quality_metrics,\n",
    "                model, device\n",
    "            )\n",
    "            benchmark_results_list.append({\n",
    "                \"text\": text_sample,\n",
    "                \"cfg_scale\": cfg_val,\n",
    "                \"rtf\": rtf,\n",
    "                \"audio_duration\": audio_dur,\n",
    "                \"generation_time\": gen_time,\n",
    "                \"audio_file\": audio_file\n",
    "            })\n",
    "\n",
    "    print(\"\\n\\n--- Benchmark Summary ---\")\n",
    "    table_header = f\"{'CFG':<5} | {'Text Len':<8} | {'RTF':<5} | {'Audio (s)':<10} | {'Gen Time (s)':<12} | {'File':<70}\"\n",
    "    print(table_header)\n",
    "    print(\"-\" * len(table_header))\n",
    "    for res in benchmark_results_list:\n",
    "        text_len_desc = \"Short\" if len(res['text']) < 20 else \"Medium\" if len(res['text']) < 70 else \"Long\"\n",
    "        print(f\"{res['cfg_scale']:<5.1f} | {text_len_desc:<8} | {res['rtf']:<5.2f} | {res['audio_duration']:<10.2f} | {res['generation_time']:<12.2f} | {os.path.basename(res['audio_file']):<70}\")\n",
    "        ipd.display(ipd.HTML(f\"<b>Text:</b> {res['text']}<br><b>CFG:</b> {res['cfg_scale']}, <b>File:</b> {res['audio_file']}\"))\n",
    "        ipd.display(ipd.Audio(res['audio_file']))\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "    globals()['benchmark_run_results_list'] = benchmark_results_list\n",
    "\n",
    "print(\"\\nâœ… Benchmarking cell execution complete.\")\n",
    "print(\"Reminder: If you've updated zonos/model.py or other core files, ensure you've re-run Cell 3 to load changes before running Cell 5 or this benchmark cell.\")"
   ]
  },
  { # Original Cell 7 (Summary) - Now Index 7
   "cell_type": "markdown",
   "metadata": {
    "id": "summary_header"
   },
   "source": [
    "---\n",
    "## ðŸŽ‰ Enhanced Voice Cloning Complete!\n",
    "\n",
    "You've successfully used the enhanced voice cloning system with Zonos TTS. This notebook provides a comprehensive suite for voice cloning, generation, and performance benchmarking.\n",
    "\n",
    "### ðŸš€ What's Enhanced & Key Features:\n",
    "- **Improved Speech Quality**: Significant reductions in gibberish, better timing consistency, and more natural speech flow.\n",
    "- **Advanced Audio Preprocessing**: Automatic silence removal and normalization for uploaded voice samples.\n",
    "- **Voice Quality Analysis**: SNR estimation and quality scoring for your voice samples to guide parameter choices.\n",
    "- **Flexible Quality Presets**: \n",
    "    - Choose from presets like \"Conservative\", \"Balanced\", \"Fast (Less Expressive)\", \"Expressive\", and \"Creative\".\n",
    "    - \"Fast\" preset uses a lower CFG Scale (1.5) for quicker generation with a trade-off in expressiveness.\n",
    "    - \"Expressive\" and \"Creative\" presets now incorporate specific emotion vectors for more vivid speech.\n",
    "- **Adaptive Settings**: Parameters automatically adjust based on your voice sample's quality and chosen preset.\n",
    "- **CFG Scale Control**: Support for `cfg_scale=1.0` (and other values) in `zonos.model.py` allows for fine-tuning the balance between speed and expressiveness. This is benchmarked in Cell 6.\n",
    "- **Long Text Handling**: Cell 5 now automatically segments very long texts and generates audio in chunks using audio prefixing for continuity.\n",
    "- **Reproducible Results**: Seed support for consistent audio generation.\n",
    "- **Google Colab Compatibility**: Streamlined setup and dependency management within the Colab environment.\n",
    "- **Benchmarking Tools**: Cell 7 allows for systematic testing of different CFG Scales to understand performance and quality trade-offs.\n",
    "\n",
    "### ðŸ’¡ Tips for Best Results:\n",
    "- Use clean, high-quality audio (16kHz+ sample rate, minimal background noise/music) for voice cloning.\n",
    "- Provide 10-20 seconds of clear speech for optimal cloning.\n",
    "- Experiment with different Quality Presets in Cell 6 to find the best match for your needs.\n",
    "- If modifying underlying code (like `zonos/model.py`), always re-run Cell 3 (Load Model) and Cell 4 (Helper Functions) to apply changes.\n",
    "\n",
    "### ðŸ”§ If You Encountered Issues:\n",
    "- **NumPy or other dependency errors**: Try `Runtime` > `Restart runtime` (or `Factory reset runtime`) then re-run cells from the beginning (Cell 1 onwards).\n",
    "- **`generate_long_audio_from_text` not defined**: Ensure Cell 4 (Helper Functions) has been run successfully.\n",
    "- **Model loading errors after code changes**: Ensure you've re-run Cell 3.\n",
    "- **Memory errors**: Try shorter text for generation or restart the runtime.\n",
    "- **Audio quality issues**: Use cleaner source audio for cloning. Experiment with different presets in Cell 6.\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ¤ Thank you for using Enhanced Voice Cloning with Zonos TTS!**\n",
    "\n",
    "For more information, visit: [Zonos GitHub Repository](https://github.com/Wamp1re-Ai/Zonos)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
