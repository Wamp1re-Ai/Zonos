{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54137773",
   "metadata": {},
   "source": [
    "# Zonos Model: Google Colab Notebook\n",
    "\n",
    "This notebook will help you set up and run the Zonos model in Google Colab. The model will be automatically downloaded from [Hugging Face](https://huggingface.co/Wamp1re-Ai/Zonos-v0.1-transformer).\n",
    "\n",
    "## Steps:\n",
    "1. Clone or upload the Zonos repository and download the model from HuggingFace.\n",
    "2. Install dependencies.\n",
    "3. Run the model with example code.\n",
    "4. (Optional) Upload your own audio files for inference.\n",
    "5. Use Cloudflare subdomain for sharing your Gradio interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade15647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. Clone the repository and download the model\n",
    "import os\n",
    "\n",
    "# Clone the Zonos repository (use the correct URL)\n",
    "if not os.path.exists('Zonos'):\n",
    "    !git clone https://github.com/Wamp1re-Ai/Zonos.git  # Change this to your actual GitHub username\n",
    "    print(\"Repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"Repository already exists!\")\n",
    "\n",
    "%cd Zonos\n",
    "\n",
    "# Install system dependencies first (eSpeak is required for phonemization)\n",
    "!apt-get update -qq\n",
    "!apt-get install -y espeak-ng git-lfs\n",
    "\n",
    "# Initialize git LFS\n",
    "!git lfs install\n",
    "\n",
    "print(\"System dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b350a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2. Install dependencies\n",
    "# Install required packages efficiently and avoid dependency conflicts\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package with better error handling\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to install {package}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Don't upgrade pip/setuptools in Colab - it causes conflicts\n",
    "print(\"Skipping pip upgrade to avoid dependency conflicts in Colab...\")\n",
    "\n",
    "# Check if we're in Colab and use pre-installed torch if available\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Use Colab's pre-installed PyTorch to save time\n",
    "    print(\"Using Colab's pre-installed PyTorch...\")\n",
    "    try:\n",
    "        import torch\n",
    "        import torchaudio\n",
    "        print(f\"‚úì PyTorch {torch.__version__} already available\")\n",
    "        print(f\"‚úì TorchAudio {torchaudio.__version__} already available\")\n",
    "        torch_installed = True\n",
    "    except ImportError:\n",
    "        print(\"PyTorch not found, will install...\")\n",
    "        torch_installed = False\n",
    "else:\n",
    "    torch_installed = False\n",
    "\n",
    "# Install core dependencies (skip torch if already available)\n",
    "packages = [\n",
    "    \"transformers>=4.45.0\",\n",
    "    \"gradio>=4.0.0\", \n",
    "    \"huggingface-hub>=0.20.0\",\n",
    "    \"soundfile>=0.12.1\",\n",
    "    \"phonemizer>=3.2.0\",\n",
    "    \"numpy>=1.24.0\",\n",
    "    \"inflect>=7.0.0\",\n",
    "    \"scipy\"\n",
    "]\n",
    "\n",
    "# Add torch packages if not already installed\n",
    "if not torch_installed:\n",
    "    packages = [\"torch>=2.0.0\", \"torchaudio>=2.0.0\"] + packages\n",
    "\n",
    "print(f\"Installing {len(packages)} core dependencies...\")\n",
    "failed_packages = []\n",
    "\n",
    "for package in packages:\n",
    "    print(f\"Installing {package}...\")\n",
    "    if not install_package(package):\n",
    "        failed_packages.append(package)\n",
    "\n",
    "if failed_packages:\n",
    "    print(f\"\\n‚ö†Ô∏è Failed to install: {failed_packages}\")\n",
    "    print(\"Continuing anyway - some packages might work...\")\n",
    "\n",
    "# Install optional dependencies for better performance (if supported)\n",
    "print(\"\\nInstalling optional dependencies...\")\n",
    "optional_packages = [\n",
    "    \"flash-attn>=2.0.0\",\n",
    "    \"mamba-ssm>=1.0.0\", \n",
    "    \"causal-conv1d>=1.0.0\"\n",
    "]\n",
    "\n",
    "for package in optional_packages:\n",
    "    print(f\"Attempting to install {package}...\")\n",
    "    if not install_package(package):\n",
    "        print(f\"  -> {package} installation failed (optional - continuing)\")\n",
    "\n",
    "# Install the project itself\n",
    "print(\"\\nInstalling Zonos package...\")\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \".\", \"--quiet\"])\n",
    "    print(\"‚úì Zonos package installed successfully!\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚ùå Failed to install Zonos package: {e}\")\n",
    "    print(\"Trying alternative installation...\")\n",
    "    # Alternative: add current directory to Python path\n",
    "    current_dir = os.getcwd()\n",
    "    if current_dir not in sys.path:\n",
    "        sys.path.insert(0, current_dir)\n",
    "    print(f\"Added {current_dir} to Python path\")\n",
    "\n",
    "print(\"\\n‚úÖ Dependency installation complete!\")\n",
    "print(\"\\nüìù Installation Summary:\")\n",
    "print(f\"  - Core packages: {len(packages) - len(failed_packages)}/{len(packages)} successful\")\n",
    "if failed_packages:\n",
    "    print(f\"  - Failed packages: {failed_packages}\")\n",
    "print(f\"  - Optional packages: Attempted (failures are normal)\")\n",
    "print(f\"  - Zonos package: Installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3. Load and run the Zonos model\n",
    "import sys\n",
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "\n",
    "# Make sure we can import zonos modules\n",
    "if '/content/Zonos' not in sys.path:\n",
    "    sys.path.insert(0, '/content/Zonos')\n",
    "\n",
    "try:\n",
    "    from zonos.model import Zonos, DEFAULT_BACKBONE_CLS\n",
    "    from zonos.conditioning import make_cond_dict, supported_language_codes\n",
    "    from zonos.utils import DEFAULT_DEVICE\n",
    "    print(\"‚úì Zonos modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Make sure the previous installation steps completed successfully.\")\n",
    "    raise\n",
    "\n",
    "# Set device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check GPU memory if using CUDA\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Load the model from HuggingFace\n",
    "model_name = \"Wamp1re-Ai/Zonos-v0.1-transformer\"\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(\"This may take a few minutes for the first time...\")\n",
    "\n",
    "try:\n",
    "    model = Zonos.from_pretrained(model_name, device=device)\n",
    "    model.requires_grad_(False).eval()\n",
    "    print(\"‚úì Model loaded successfully!\")\n",
    "    \n",
    "    # Show available conditioning options\n",
    "    if hasattr(model, 'prefix_conditioner') and hasattr(model.prefix_conditioner, 'conditioners'):\n",
    "        print(\"\\nAvailable conditioning options:\")\n",
    "        for c in model.prefix_conditioner.conditioners:\n",
    "            print(f\"  - {c.name}\")\n",
    "    \n",
    "    # Show supported languages\n",
    "    print(f\"\\nSupported languages: {supported_language_codes}\")\n",
    "            \n",
    "    print(\"\\nüéâ Setup complete! You can now use the model in the cells below.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"1. Make sure you have a stable internet connection\")\n",
    "    print(\"2. Check if you have enough GPU/RAM memory\")\n",
    "    print(\"3. Try restarting the runtime and running from the beginning\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b30930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. Run text-to-speech example\n",
    "\n",
    "# Import necessary modules for conditional generation\n",
    "from google.colab import files\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "\n",
    "#@markdown ### Text and language settings\n",
    "text = \"Hello, this is Zonos text-to-speech model. How can I help you today?\" #@param {type:\"string\"}\n",
    "language = \"en-us\" #@param [\"en-us\", \"en-gb\", \"fr-fr\", \"es-es\", \"de-de\", \"it-it\", \"ja-jp\", \"zh-cn\"]\n",
    "\n",
    "#@markdown ### Optional: Upload your own audio for speaker cloning\n",
    "use_speaker_cloning = False #@param {type:\"boolean\"}\n",
    "\n",
    "speaker_embedding = None\n",
    "if use_speaker_cloning:\n",
    "    print(\"Upload a short audio file (5-30 seconds) of the speaker you want to clone:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    if uploaded:\n",
    "        speaker_file = list(uploaded.keys())[0]\n",
    "        print(f\"Processing {speaker_file}...\")\n",
    "        try:\n",
    "            wav, sr = torchaudio.load(speaker_file)\n",
    "            # Convert to mono if stereo\n",
    "            if wav.shape[0] > 1:\n",
    "                wav = wav.mean(0, keepdim=True)\n",
    "            speaker_embedding = model.make_speaker_embedding(wav, sr)\n",
    "            speaker_embedding = speaker_embedding.to(device, dtype=torch.bfloat16)\n",
    "            print(f\"‚úì Speaker embedding created from {speaker_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing speaker audio: {e}\")\n",
    "            use_speaker_cloning = False\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create conditioning dictionary\n",
    "print(\"Creating conditioning...\")\n",
    "try:\n",
    "    cond_dict = make_cond_dict(\n",
    "        text=text,\n",
    "        language=language,\n",
    "        speaker=speaker_embedding,\n",
    "        device=device,\n",
    "        # Use emotion as unconditional for more natural speech\n",
    "        unconditional_keys=[\"emotion\"] if not use_speaker_cloning else [\"emotion\"]\n",
    "    )\n",
    "\n",
    "    # Prepare conditioning\n",
    "    conditioning = model.prepare_conditioning(cond_dict)\n",
    "\n",
    "    # Generate audio\n",
    "    print(\"üéµ Generating audio...\")\n",
    "    print(\"This may take 30-60 seconds depending on text length...\")\n",
    "    \n",
    "    codes = model.generate(\n",
    "        prefix_conditioning=conditioning,\n",
    "        max_new_tokens=min(86 * 30, len(text) * 20),  # Adaptive based on text length\n",
    "        cfg_scale=2.0,\n",
    "        batch_size=1,\n",
    "        progress_bar=True\n",
    "    )\n",
    "\n",
    "    # Decode the audio\n",
    "    print(\"üîä Decoding audio...\")\n",
    "    wav_out = model.autoencoder.decode(codes).cpu().detach()\n",
    "    sr_out = model.autoencoder.sampling_rate\n",
    "    \n",
    "    if wav_out.dim() == 2 and wav_out.size(0) > 1:\n",
    "        wav_out = wav_out[0:1, :]\n",
    "\n",
    "    # Play the audio\n",
    "    wav_numpy = wav_out.squeeze().numpy()\n",
    "    print(f\"‚úì Audio generated successfully!\")\n",
    "    print(f\"Sample rate: {sr_out} Hz, Duration: {len(wav_numpy)/sr_out:.2f} seconds\")\n",
    "    \n",
    "    # Display audio player\n",
    "    ipd.display(ipd.Audio(wav_numpy, rate=sr_out))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during audio generation: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"- Try shorter text (under 100 characters)\")\n",
    "    print(\"- Check GPU memory usage\")\n",
    "    print(\"- Restart runtime if needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38930fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.1 Advanced Text-to-Speech Options\n",
    "\n",
    "#@markdown ### Adjust model parameters for generation\n",
    "\n",
    "#@markdown #### Text and language\n",
    "text = \"I can speak with different emotions and characteristics. This is an example of advanced text-to-speech synthesis.\" #@param {type:\"string\"}\n",
    "language = \"en-us\" #@param [\"en-us\", \"en-gb\", \"fr-fr\", \"es-es\", \"de-de\", \"it-it\", \"ja-jp\", \"zh-cn\"]\n",
    "\n",
    "#@markdown #### Emotion controls (0-1 scale)\n",
    "happiness = 0.7 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "sadness = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "anger = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "fear = 0.05 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "surprise = 0.05 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "disgust = 0.05 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "other = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "neutral = 0.3 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "\n",
    "#@markdown #### Voice characteristics  \n",
    "speaking_rate = 15.0 #@param {type:\"slider\", min:5, max:30, step:0.5}\n",
    "pitch_std = 45.0 #@param {type:\"slider\", min:0, max:300, step:5}\n",
    "fmax = 24000 #@param {type:\"slider\", min:8000, max:24000, step:1000}\n",
    "vq_score = 0.78 #@param {type:\"slider\", min:0.5, max:0.8, step:0.01}\n",
    "dnsmos_ovrl = 4.0 #@param {type:\"slider\", min:1, max:5, step:0.1}\n",
    "\n",
    "#@markdown #### Generation settings\n",
    "cfg_scale = 2.0 #@param {type:\"slider\", min:1, max:5, step:0.1}\n",
    "randomize_seed = True #@param {type:\"boolean\"}\n",
    "seed = 42 #@param {type:\"integer\"}\n",
    "max_length_multiplier = 20 #@param {type:\"slider\", min:10, max:50, step:5}\n",
    "\n",
    "# Set seed for reproducibility\n",
    "if not randomize_seed:\n",
    "    torch.manual_seed(seed)\n",
    "    used_seed = seed\n",
    "else:\n",
    "    used_seed = torch.randint(0, 2**32 - 1, (1,)).item()\n",
    "    torch.manual_seed(used_seed)\n",
    "\n",
    "print(f\"Using seed: {used_seed}\")\n",
    "\n",
    "# Validate emotion values sum (should be close to 1.0 for best results)\n",
    "emotion_sum = happiness + sadness + anger + fear + surprise + disgust + other + neutral\n",
    "if emotion_sum > 1.2 or emotion_sum < 0.8:\n",
    "    print(f\"‚ö†Ô∏è Warning: Emotion values sum to {emotion_sum:.2f}, consider adjusting for better results\")\n",
    "\n",
    "try:\n",
    "    # Create emotion tensor\n",
    "    emotion_tensor = torch.tensor([\n",
    "        float(happiness),  # Happiness\n",
    "        float(sadness),    # Sadness  \n",
    "        float(disgust),    # Disgust\n",
    "        float(fear),       # Fear\n",
    "        float(surprise),   # Surprise\n",
    "        float(anger),      # Anger\n",
    "        float(other),      # Other\n",
    "        float(neutral)     # Neutral\n",
    "    ], device=device)\n",
    "\n",
    "    # Create VQ score tensor (8 values for 8 codebooks)\n",
    "    vq_tensor = torch.tensor([float(vq_score)] * 8, device=device).unsqueeze(0)\n",
    "\n",
    "    # Create conditioning dictionary with more parameters\n",
    "    print(\"Creating advanced conditioning...\")\n",
    "    cond_dict = make_cond_dict(\n",
    "        text=text,\n",
    "        language=language,\n",
    "        speaker=speaker_embedding if 'speaker_embedding' in globals() else None,\n",
    "        emotion=emotion_tensor,\n",
    "        speaking_rate=speaking_rate,\n",
    "        pitch_std=pitch_std,\n",
    "        fmax=fmax,\n",
    "        vqscore_8=vq_tensor,\n",
    "        dnsmos_ovrl=dnsmos_ovrl,\n",
    "        device=device,\n",
    "        unconditional_keys=[\"emotion\"] if 'speaker_embedding' not in globals() or speaker_embedding is None else []\n",
    "    )\n",
    "\n",
    "    # Prepare conditioning\n",
    "    conditioning = model.prepare_conditioning(cond_dict)\n",
    "\n",
    "    # Calculate appropriate max_new_tokens based on text length\n",
    "    estimated_tokens = min(86 * 30, len(text) * max_length_multiplier)\n",
    "    \n",
    "    # Generate audio\n",
    "    print(f\"üéµ Generating audio with advanced settings...\")\n",
    "    print(f\"Text length: {len(text)} chars, Estimated tokens: {estimated_tokens}\")\n",
    "    \n",
    "    codes = model.generate(\n",
    "        prefix_conditioning=conditioning,\n",
    "        max_new_tokens=estimated_tokens,\n",
    "        cfg_scale=cfg_scale,\n",
    "        batch_size=1,\n",
    "        progress_bar=True,\n",
    "        sampling_params=dict(min_p=0.1, top_k=0, top_p=0.0)  # Use min_p sampling\n",
    "    )\n",
    "\n",
    "    # Decode the audio\n",
    "    print(\"üîä Decoding audio...\")\n",
    "    wav_out = model.autoencoder.decode(codes).cpu().detach()\n",
    "    sr_out = model.autoencoder.sampling_rate\n",
    "    if wav_out.dim() == 2 and wav_out.size(0) > 1:\n",
    "        wav_out = wav_out[0:1, :]\n",
    "\n",
    "    # Play the audio\n",
    "    wav_numpy = wav_out.squeeze().numpy()\n",
    "    duration = len(wav_numpy) / sr_out\n",
    "    \n",
    "    print(f\"‚úì Advanced audio generated successfully!\")\n",
    "    print(f\"Sample rate: {sr_out} Hz, Duration: {duration:.2f} seconds\")\n",
    "    print(f\"Settings used: CFG={cfg_scale}, Emotions=[H:{happiness}, S:{sadness}, A:{anger}, N:{neutral}]\")\n",
    "    ipd.display(ipd.Audio(wav_numpy, rate=sr_out))\n",
    "    \n",
    "    # Store for potential download\n",
    "    globals()['last_generated_audio'] = (wav_numpy, sr_out, used_seed)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during advanced audio generation: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"- Try simpler emotion settings (closer to default values)\")\n",
    "    print(\"- Reduce text length\")\n",
    "    print(\"- Lower CFG scale (try 1.5-2.0)\")\n",
    "    print(\"- Check GPU memory usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af95e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.2 Save Generated Audio\n",
    "\n",
    "#@markdown ### Save and download the generated audio\n",
    "import os\n",
    "import scipy.io.wavfile\n",
    "from datetime import datetime\n",
    "\n",
    "#@markdown Choose what to save\n",
    "save_last_generated = True #@param {type:\"boolean\"}\n",
    "filename_prefix = \"zonos_audio\" #@param {type:\"string\"}\n",
    "include_timestamp = True #@param {type:\"boolean\"}\n",
    "include_settings = True #@param {type:\"boolean\"}\n",
    "\n",
    "# Check if we have generated audio from previous cells\n",
    "try:\n",
    "    if 'last_generated_audio' in globals():\n",
    "        wav_numpy, sr_out, used_seed = last_generated_audio\n",
    "        \n",
    "        # Create filename\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") if include_timestamp else \"\"\n",
    "        settings_suffix = f\"_seed{used_seed}\" if include_settings else \"\"\n",
    "        \n",
    "        if timestamp and settings_suffix:\n",
    "            filename = f\"{filename_prefix}_{timestamp}{settings_suffix}.wav\"\n",
    "        elif timestamp:\n",
    "            filename = f\"{filename_prefix}_{timestamp}.wav\"\n",
    "        elif settings_suffix:\n",
    "            filename = f\"{filename_prefix}{settings_suffix}.wav\"\n",
    "        else:\n",
    "            filename = f\"{filename_prefix}.wav\"\n",
    "        \n",
    "        # Save the audio file\n",
    "        print(f\"üíæ Saving audio as: {filename}\")\n",
    "        scipy.io.wavfile.write(filename, sr_out, wav_numpy)\n",
    "        \n",
    "        # Show file info\n",
    "        file_size = os.path.getsize(filename) / (1024 * 1024)  # MB\n",
    "        duration = len(wav_numpy) / sr_out\n",
    "        print(f\"‚úì Audio saved successfully!\")\n",
    "        print(f\"  File: {filename}\")\n",
    "        print(f\"  Size: {file_size:.2f} MB\")\n",
    "        print(f\"  Duration: {duration:.2f} seconds\")\n",
    "        print(f\"  Sample rate: {sr_out} Hz\")\n",
    "        \n",
    "        # Provide download link\n",
    "        print(\"\\nüì• Starting download...\")\n",
    "        from google.colab import files\n",
    "        files.download(filename)\n",
    "        \n",
    "        print(\"üéâ Audio file ready for download!\")\n",
    "        \n",
    "    elif 'wav_numpy' in globals() and 'sr_out' in globals():\n",
    "        # Fallback to basic variables if available\n",
    "        filename = f\"{filename_prefix}_basic.wav\"\n",
    "        scipy.io.wavfile.write(filename, sr_out, wav_numpy)\n",
    "        print(f\"‚úì Audio saved as {filename}\")\n",
    "        files.download(filename)\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No audio has been generated yet.\")\n",
    "        print(\"Run one of the audio generation cells above first.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving audio: {e}\")\n",
    "    print(\"Make sure audio generation completed successfully in previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd2978",
   "metadata": {},
   "source": [
    "---\n",
    "**Note:**\n",
    "- Adjust the import paths and model usage according to your codebase.\n",
    "- If you encounter issues with dependencies, check the `pyproject.toml` or manually install missing packages.\n",
    "\n",
    "## Additional Notes and Troubleshooting\n",
    "\n",
    "### ‚úÖ What This Notebook Does\n",
    "- Automatically installs all required dependencies including system packages\n",
    "- Downloads the Zonos transformer model from HuggingFace (2-3 GB)\n",
    "- Provides both simple and advanced text-to-speech generation\n",
    "- Supports speaker cloning with uploaded audio files\n",
    "- Includes a full Gradio web interface for interactive use\n",
    "- Handles error checking and provides helpful feedback\n",
    "\n",
    "### üéØ Performance Tips\n",
    "- **GPU Runtime**: Use GPU runtime for best performance (Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU)\n",
    "- **Text Length**: Shorter texts (under 200 characters) generate faster\n",
    "- **Memory**: The model uses ~2-4 GB of GPU memory when loaded\n",
    "- **First Run**: Initial model download may take 5-10 minutes depending on connection\n",
    "\n",
    "### üîß Common Issues and Solutions\n",
    "\n",
    "**Model Loading Fails:**\n",
    "- Check internet connection stability\n",
    "- Ensure sufficient GPU/RAM memory (restart runtime if needed)\n",
    "- Try the transformer model if hybrid fails\n",
    "\n",
    "**Audio Generation Errors:**\n",
    "- Reduce text length (try under 100 characters)\n",
    "- Lower CFG scale (try 1.5 instead of 2.0)\n",
    "- Simplify emotion settings (use defaults)\n",
    "- Check GPU memory usage\n",
    "\n",
    "**Import Errors:**\n",
    "- Restart runtime and run all cells from the beginning\n",
    "- Check that installation completed without errors\n",
    "- Ensure you're using a GPU runtime\n",
    "\n",
    "**Speaker Cloning Issues:**\n",
    "- Use audio files 5-30 seconds long\n",
    "- Ensure audio is clear and contains speech\n",
    "- Supported formats: WAV, MP3, FLAC\n",
    "- Try mono audio if stereo doesn't work\n",
    "\n",
    "### üìö Model Information\n",
    "- **Model**: Wamp1re-Ai/Zonos-v0.1-transformer\n",
    "- **Languages**: English, Japanese, Chinese, French, German\n",
    "- **Sample Rate**: 44.1 kHz\n",
    "- **Architecture**: Transformer-based with DAC autoencoder\n",
    "- **Training Data**: 200k+ hours of multilingual speech\n",
    "\n",
    "### üåê Using Custom Subdomains\n",
    "When running the Gradio interface, you can optionally use a custom Cloudflare subdomain:\n",
    "1. Set `use_custom_subdomain = True` in the Gradio cell\n",
    "2. Choose a unique subdomain name\n",
    "3. Your interface will be available at `https://your-name.gradio.app`\n",
    "\n",
    "### üí° Advanced Usage\n",
    "For production use or custom applications, consider:\n",
    "- Using the hybrid model for better quality (requires mamba-ssm)\n",
    "- Implementing custom conditioning parameters\n",
    "- Fine-tuning for specific voices or languages\n",
    "- Using the API programmatically\n",
    "\n",
    "### üîó Useful Links\n",
    "- [Zonos GitHub Repository](https://github.com/YourUsername/Zonos)\n",
    "- [Model on HuggingFace](https://huggingface.co/Wamp1re-Ai/Zonos-v0.1-transformer)\n",
    "- [Zyphra Blog Post](https://www.zyphra.com/post/beta-release-of-zonos-v0-1)\n",
    "- [Online Playground](https://playground.zyphra.com/audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494540d0",
   "metadata": {},
   "source": [
    "## Using Cloudflare Subdomain for Gradio Interface\n",
    "\n",
    "When running the Gradio interface, you can use a custom Cloudflare subdomain to make your interface accessible via a consistent URL. This is especially useful for sharing your model with others.\n",
    "\n",
    "**To use a custom subdomain:**\n",
    "\n",
    "1. Set the `GRADIO_SUBDOMAIN` environment variable to your desired subdomain name.\n",
    "2. Set `GRADIO_SHARE=True` to enable sharing.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5. Run Gradio Interface (Interactive Web UI)\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "\n",
    "#@markdown ### Gradio Interface Settings\n",
    "subdomain_name = \"my-zonos-app\" #@param {type:\"string\"}\n",
    "use_custom_subdomain = False #@param {type:\"boolean\"}\n",
    "share_publicly = True #@param {type:\"boolean\"}\n",
    "\n",
    "# Set environment variables for Gradio\n",
    "if use_custom_subdomain and subdomain_name:\n",
    "    os.environ[\"GRADIO_SUBDOMAIN\"] = subdomain_name\n",
    "    print(f\"üåê Will attempt to use subdomain: {subdomain_name}.gradio.app\")\n",
    "else:\n",
    "    # Remove subdomain if previously set\n",
    "    os.environ.pop(\"GRADIO_SUBDOMAIN\", None)\n",
    "\n",
    "os.environ[\"GRADIO_SHARE\"] = \"True\" if share_publicly else \"False\"\n",
    "\n",
    "print(\"üöÄ Starting Gradio interface...\")\n",
    "print(\"This may take a moment to initialize...\")\n",
    "\n",
    "# Check if gradio_interface.py exists\n",
    "if not os.path.exists(\"gradio_interface.py\"):\n",
    "    print(\"‚ùå gradio_interface.py not found!\")\n",
    "    print(\"Make sure you're in the correct directory and the file exists.\")\n",
    "else:\n",
    "    try:\n",
    "        # Import and run the interface\n",
    "        import subprocess\n",
    "        import sys\n",
    "        \n",
    "        # Run the gradio interface in a separate process\n",
    "        print(\"üì± Launching Gradio interface...\")\n",
    "        print(\"Click on the public URL below to access the web interface\")\n",
    "        print(\"‚ö†Ô∏è Note: The interface will run until you stop this cell\\n\")\n",
    "        \n",
    "        # Run the gradio interface\n",
    "        result = subprocess.run([\n",
    "            sys.executable, \"gradio_interface.py\"\n",
    "        ], capture_output=False, text=True)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nüõë Gradio interface stopped by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running Gradio interface: {e}\")\n",
    "        print(\"\\nTroubleshooting:\")\n",
    "        print(\"- Make sure all dependencies are installed correctly\")\n",
    "        print(\"- Check that the model loaded successfully in previous cells\")\n",
    "        print(\"- Try restarting the runtime if needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda40922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üß™ Quick Test - Verify Everything Works\n",
    "#@markdown Run this cell to verify that Zonos is properly installed and working\n",
    "\n",
    "print(\"Running comprehensive test of Zonos installation...\")\n",
    "print(\"This will check dependencies and try to load the model.\")\n",
    "print()\n",
    "\n",
    "exec(open('colab_quick_test.py').read())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
